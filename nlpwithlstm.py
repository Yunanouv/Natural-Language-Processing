# -*- coding: utf-8 -*-
"""NLPwithLSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rUW76iQXiwuH3dCw4uO9FgCdVOX8iwPJ

**Analisis Prediksi Jenis Produk Berdasarkan Review yang Diberikan**
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/MyDrive/Dicoding/Sub1-Kaggle-Women'sClothingReview/Kaggle-Women'sClothingReviews.zip" -d "/content/drive/MyDrive/Dicoding/Sub1-Kaggle-Women'sClothingReview"

import pandas as pd
dataset = pd.read_csv("/content/drive/MyDrive/Dicoding/Sub1-Kaggle-Women'sClothingReview/Womens Clothing E-Commerce Reviews.csv")

# menghapus data yang mengandung nilai null
dataset.dropna(axis=0, subset=['Review Text','Department Name'], inplace=True)
# menghapus kolom yang tidak relevan
dataset = dataset.drop(columns=['Unnamed: 0','Title','Clothing ID','Age','Recommended IND','Division Name','Positive Feedback Count'])

dataset.head()

dataset[90:100]

import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')


dataset = dataset.reset_index(drop=True)
REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    """
        text : a string

        return : modified initial string
    """
    text = str(text).lower()
    text = REPLACE_BY_SPACE_RE.sub(' ', text)
    text = BAD_SYMBOLS_RE.sub('', text)
    text = text.replace('x', '')
    text = ' '.join(word for word in text.split() if word not in STOPWORDS)
    return text
dataset['Review Text'] = dataset['Review Text'].apply(clean_text)

dataset['Review Text'] = dataset['Review Text'].str.replace('\d+', '')

contractions = {
"ain't": "am not",
"aren't": "are not",
"can't": "cannot",
"can't've": "cannot have",
"'cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he would",
"he'd've": "he would have",
"he'll": "he will",
"he'll've": "he will have",
"he's": "he is",
"how'd": "how did",
"how'd'y": "how do you",
"how'll": "how will",
"how's": "how does",
"i'd": "i would",
"i'd've": "i would have",
"i'll": "i will",
"i'll've": "i will have",
"i'm": "i am",
"i've": "i have",
"isn't": "is not",
"it'd": "it would",
"it'd've": "it would have",
"it'll": "it will",
"it'll've": "it will have",
"it's": "it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"mightn't've": "might not have",
"must've": "must have",
"mustn't": "must not",
"mustn't've": "must not have",
"needn't": "need not",
"needn't've": "need not have",
"o'clock": "of the clock",
"oughtn't": "ought not",
"oughtn't've": "ought not have",
"shan't": "shall not",
"sha'n't": "shall not",
"shan't've": "shall not have",
"she'd": "she would",
"she'd've": "she would have",
"she'll": "she will",
"she'll've": "she will have",
"she's": "she is",
"should've": "should have",
"shouldn't": "should not",
"shouldn't've": "should not have",
"so've": "so have",
"so's": "so is",
"that'd": "that would",
"that'd've": "that would have",
"that's": "that is",
"there'd": "there would",
"there'd've": "there would have",
"there's": "there is",
"they'd": "they would",
"they'd've": "they would have",
"they'll": "they will",
"they'll've": "they will have",
"they're": "they are",
"they've": "they have",
"to've": "to have",
"wasn't": "was not",
" u ": " you ",
" ur ": " your ",
" n ": " and "}

dataset['Department Name'].value_counts()

dataset = dataset.loc[dataset['Department Name'].isin(["Tops","Bottoms"])]
grouped_review = dataset.groupby(["Class Name"])['Review Text'].apply(' ::: '.join).reset_index()


# Mendapatkan kata kunci
topWear = [ "top","blouse","shirt","upper","dress","torso","tank","sleeve","body","sweater"]
bottomWear = ["pant","jean","slack","skirt","leg","waist","lower","thigh","trouser","flare"]

wordnet_lemmatizer = WordNetLemmatizer()
keyWords = topWear + bottomWear
dataFiltered = []
for each_review in dataset :
    temp = []
    for word in each_review :
        if not word in STOPWORDS and word in keyWords:
            temp.append(wordnet_lemmatizer.lemmatize(word))
    dataFiltered.append(temp)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

max_words = 10000
max_length = 250
embedding_dim = 100
tokenizer = Tokenizer(num_words=max_words, filters = '!"@#$%^&*(|[\]);<=>_{?}', lower=True, char_level=False)
tokenizer.fit_on_texts(dataset['Review Text'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens' % len(word_index))

X = tokenizer.texts_to_sequences(dataset['Review Text'].values)
X = pad_sequences(X, maxlen=max_length)
print('Shape of data tensor', X.shape)

Y = pd.get_dummies(dataset['Department Name']).values
print('Shape of Label Tensor :', Y.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)
print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Embedding, Bidirectional, SpatialDropout1D, LSTM, Dense

model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length=X.shape[1]))
model.add(Bidirectional(LSTM(150, return_sequences=True)))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))
model.add(Dense(2, activation='softmax'))
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model.summary())

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.97 and
    logs.get('val_accuracy') > 0.97 ):
      print("\Akurasi sudah mencapai 97%")
      self.model.stop_training = True
callbacks = myCallback()

epochs = 20
batch_size = 256

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=2, callbacks=[callbacks])

import matplotlib.pyplot as plt

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show();